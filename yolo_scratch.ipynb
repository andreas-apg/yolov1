{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8cc60a-c486-4a25-8486-09c36e3ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os # dataset\n",
    "import pandas as pd # dataset\n",
    "from PIL import Image # dataset\n",
    "import torchvision.transforms as transforms # train\n",
    "import torch.optim as optim # train\n",
    "import torchvision.transforms.functional as FT # train\n",
    "#from tqdm import tqdm # train\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torch.utils.data import DataLoader # train\n",
    "import numpy as np # box utils\n",
    "import matplotlib.pyplot as plt # box utils\n",
    "import matplotlib.patches as patches # box utils\n",
    "from collections import Counter # box utils\n",
    "\n",
    "import albumentations as A # train\n",
    "from albumentations.pytorch import ToTensorV2 # train\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary # main\n",
    "import matplotlib.pyplot as plt # plot functions (loss and mAP curves)\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime # main\n",
    "import pytz # main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c984a-4a21-4107-b047-798355d55c94",
   "metadata": {},
   "source": [
    "<center><img src='fig/YOLO-v1-network.png'><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5158d96-ea50-42ea-80fd-645970d36cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model\n",
    "'''\n",
    "# nout = [(nin + 2*p - k)/s] + 1\n",
    "# nin = number of input features\n",
    "# nout = number of output features\n",
    "# k = convolution kernel size\n",
    "# p = convolution padding size\n",
    "# s = convolution stride size\n",
    "architecture_config = [\n",
    "    # Tuple: (kernel_size, num_filters, stride, padding)\n",
    "    (7, 64, 2, 3), # original size\n",
    "    #(7, 64, 3, (18, 98)), # this was done to keep the 224x224 ratio after this layer\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    # List: two tuples and then last integer represents number of repeats\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "# CNN block will be a conv layer, the batch norm and then a relu\n",
    "class CNNBlock(nn.Module):\n",
    "    # our **kwargs will be split_size, num_boxes, num_classes\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        # we are going to use batchnorm, so bias = False\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "# 3 channels because of the bgr images\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        # the model in the beginning of this block\n",
    "        self.architecture = architecture_config\n",
    "        # the number of channels of the image\n",
    "        self.in_channels = in_channels\n",
    "        # the conv layers mirror what Redmon did in his darknet\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        # fully connected\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        # start_dim = 1 to not flatten the number of examples\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "    \n",
    "    def _create_conv_layers(self, architecture):\n",
    "        # all layers will be added in this empty list, created from the architecture\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        # this will go over every tuple/list in the architecture\n",
    "        for x in architecture:\n",
    "            # for a tuple, we add a CNNBlock\n",
    "            if type(x) == tuple:\n",
    "                # x arguments for a tuple being (kernel_size, num_filters, stride, padding)\n",
    "                # x[0] = kernel_size\n",
    "                # x[1] = num_filters\n",
    "                # x[2] = stride\n",
    "                # x[3] = padding\n",
    "                layers += [CNNBlock(in_channels=in_channels,\n",
    "                                  out_channels=x[1],\n",
    "                                  kernel_size=x[0],\n",
    "                                  stride=x[2],\n",
    "                                  padding=x[3],\n",
    "                                 )\n",
    "                         ]\n",
    "                # in_channels has to change for the next element\n",
    "                # to the format outputted by this tuple\n",
    "                in_channels = x[1]\n",
    "                #print(x[1])\n",
    "                \n",
    "            # for the \"M\" in the architecture a maxpool layer is added\n",
    "            elif type(x) == str:\n",
    "                # kernel_size and stride are both 2x2 and a single argument of 2\n",
    "                # is understood as (2, 2)\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            \n",
    "            # the lists in the architecture all consist of two tuples and\n",
    "            # a last integer that represents number of repeats\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0] # tuple\n",
    "                conv2 = x[1] # tuple\n",
    "                num_repeats = x[2] # integer\n",
    "\n",
    "                # the indexes of the tuple elements are the same as the singular tuples:\n",
    "                # conv[0] = kernel_size\n",
    "                # conv[1] = num_filters\n",
    "                # conv[2] = stride\n",
    "                # conv[3] = padding\n",
    "                for _ in range(num_repeats):\n",
    "                    # first we add the first tuple of the list\n",
    "                    layers += [CNNBlock(in_channels=in_channels,\n",
    "                                        out_channels=conv1[1],\n",
    "                                        kernel_size=conv1[0],\n",
    "                                        stride=conv1[2],\n",
    "                                        padding=conv1[3],\n",
    "                                        )\n",
    "                              ]\n",
    "                    # then the second tuple of the list.\n",
    "                    # the in_channels is now the output from the\n",
    "                    # FIRST conv layer, so conv1[1]\n",
    "                    layers += [CNNBlock(in_channels=conv1[1],\n",
    "                                        out_channels=conv2[1],\n",
    "                                        kernel_size=conv2[0],\n",
    "                                        stride=conv2[2],\n",
    "                                        padding=conv2[3],\n",
    "                                        )\n",
    "                              ]\n",
    "                    # we now change in_channels itself to the num_filters\n",
    "                    # of the second tuple, because it will be the input of\n",
    "                    # the first tuple the next time the loop iterates\n",
    "                    in_channels = conv2[1]\n",
    "                    \n",
    "        # we do * to unpack the list, which then will be converted to a nn.Sequential\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    # the linear fully connected layer with 4096 entries before the final output\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        # this will be reshaped later in the loss function to\n",
    "        # not be linear anymore\n",
    "        return nn.Sequential(nn.Flatten(),\n",
    "                             nn.Linear(1024 * S * S, 4096),\n",
    "                             nn.Dropout(0.0),\n",
    "                             nn.LeakyReLU(0.1),\n",
    "                             nn.Linear(4096, S * S * (C + B * 5))\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec387260-7757-4956-be04-6752d5e576ed",
   "metadata": {},
   "source": [
    "<center><img src='fig/iou.png' width=\"600\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e091407-d14e-4a47-a441-aff078a9c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IOU\n",
    "'''\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union.\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes are (x, y, w, h) or (x1, y1, x2, y2)\n",
    "    \n",
    "    Returns:\n",
    "        tensor: intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    # boxes_labels shape is (N, 4)\n",
    "    # ... means all the dimensions before the last\n",
    "    # we do this [..., 0:1] thing to slice the tensor\n",
    "    # and keep the N dimensions, so we get (N, 1)\n",
    "    if box_format == \"midpoint\":\n",
    "        # (x, y, w, h)\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "        \n",
    "    if box_format == \"corners\":\n",
    "        #(x1, x2, y1, y2)\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]    \n",
    "    \n",
    "    # if it's midpoint, we need to do calculations first.\n",
    "    # midpoint gives x and y for the center of the box, then\n",
    "    # also passes the arguments h and w for its height and\n",
    "    # width. Since we are at the center of the object, half\n",
    "    # of width and height is used to find the top, left, right\n",
    "    # and bottoms.\n",
    "    # Reminder that up left corner of an image is x, y = 0.\n",
    "    # top = y - h/2\n",
    "    # bottom = y + h/2\n",
    "    # left = x - w/2\n",
    "    # right = x + w/2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    \n",
    "    # if they don't intersect, at least one will be 0.\n",
    "    # .clamp(0) is for such edge case, because if they\n",
    "    # don't intersect, the intersection SHOULD be 0.\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y1 - box1_y2))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y1 - box2_y2))\n",
    "    \n",
    "    # the union is just the sum of the area of each box minus\n",
    "    # their intersection.\n",
    "    union = (box1_area + box2_area) - intersection\n",
    "    \n",
    "    # that 1e-6 is added for numerical stability\n",
    "    return intersection / (union + 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb97a3-97bb-4523-a08b-538107b84f72",
   "metadata": {},
   "source": [
    "<center><img src='fig/loss.png'><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2baee61-819f-4818-9beb-c9eec8c868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loss\n",
    "'''\n",
    "class YoloLoss(nn.Module):\n",
    "    # our kwargs will be split_size, num_boxes, num_classes, no_obj and coord\n",
    "    def __init__(self, split_size, num_boxes, num_classes, no_obj = 0.5, coord = 5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        # mean square error is what we use for loss\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.C = num_classes\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_no_obj = no_obj\n",
    "        self.lambda_coord = coord\n",
    "        \n",
    "    # Target:\n",
    "    # (S, S, [C] + prob_s + [x, y, w, h]) \n",
    "    # = (S, S, C + 5)\n",
    "    # The target only has one box, while YOLO predicts two boxes per cell\n",
    "    # Prediction:\n",
    "    # (S, S, [C] + prob_s1 + [x1, y1, w1, h1] + prob_s2 + [x2, y2, w2, h2])\n",
    "    # = (S, S, C + 10)\n",
    "    def forward(self, predictions, target):\n",
    "        #-1 keeps the number of examples, \n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "        # (S, S, C + 10)\n",
    "        # the probability score for box1 is at position C, so C+1:C+5 contains the box's [x, y, w, h]\n",
    "        iou_b1 = intersection_over_union(predictions[..., (self.C+1):(self.C+5)], target[..., (self.C+1):(self.C+5)])\n",
    "        # the probability score for box2 is at position C+5, so C+6:C+10 for that box's [x, y, w, h]\n",
    "        iou_b2 = intersection_over_union(predictions[..., (self.C+6):(self.C+10)], target[..., (self.C+1):(self.C+5)])\n",
    "        # torch.unsqueeze adds an additional dimension to the tensor. unsqueze(0) adds that \n",
    "        # before all others, so for example an array of size (5) would become the matrix (1, 5)\n",
    "        # torch.cat concatenates the given sequence of tensors in the given dimension.\n",
    "        # So by first unsqueezing them into (1, <tensor>) before concatenating them in the 0th\n",
    "        # dimension we are ordering the ious sequentially. iou[0] will contain iou_b1 and iou[1]\n",
    "        # will contain iou_b2. You can't concatenate tensors if you don't add that extra dimension\n",
    "        # first.\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "        # torch.max returns the result tuple of two output tensors (max, max_indices).\n",
    "        # what interest us is the best_box, not the max values from iou_maxes. This box\n",
    "        # is the iou responsible for that cell.\n",
    "        iou_maxes, best_box = torch.max(ious, dim=0)\n",
    "        # we want the probability of an object being there, which is stored at index C,\n",
    "        # after the individual class probabilities that are from 0 to C-1. This value is\n",
    "        # 0 or 1 depending if there is an object in that cell.\n",
    "        # This gives the identity function Iobj_i used in the first and fifth lines of the image.\n",
    "        # Unsqueeze is used because when acessing the Cth index, the last dimension disappears. We\n",
    "        # want to add that back.\n",
    "        exists_box = target[..., self.C].unsqueeze(3) # Iobj_i\n",
    "        #print(exists_box)\n",
    "        \n",
    "        # =================== #\n",
    "        # FOR BOX COORDINATES #\n",
    "        # =================== #\n",
    "        # lines 1 and 2 of the equations\n",
    "        \n",
    "        # best box will be 0 if box1 was correct and 1, if box2 was. We use\n",
    "        # this property to zero the tensor that has the dimensions of the wrong\n",
    "        # box, by multiplyer bext_box with [x2, y2, w2, h2] and (1 - best_box) \n",
    "        # with [x1, y1, w1, h1]. \n",
    "        # exists_box multiplies this all so predictions are only considered if THERE\n",
    "        # is a box in the first place from the \"objectness\" property.\n",
    "        box_predictions = exists_box * ((        best_box * predictions[..., (self.C+6):(self.C+10)]\n",
    "                                        + (1 - best_box) * predictions[..., (self.C+1):(self.C+5)]\n",
    "                                       ))\n",
    "        # same exists_box thing with target, but target only has one box.\n",
    "        box_targets = exists_box * target[..., (self.C+1):(self.C+5)]\n",
    "        \n",
    "        # after the previous two lines, box_predictions and box_targets now contain tensors\n",
    "        # of shape (N, S, S, 4), N being the number of input examples. By indexing [..., 2:4]\n",
    "        # we are looking at [w, h] from [x, y, w, h] for the SxS cells of each example.\n",
    "        # torch.sign is used to get the sign of the gradient, then the sqrt is taken from the\n",
    "        # absolute value because the values could be negative. If the result of the sqrt is\n",
    "        # zero, the derivative can go to infinity, so 1e-6 is used for numerical stability.\n",
    "        box_predictions[..., 2:4] = (torch.sign(box_predictions[..., 2:4]) *\n",
    "                                    torch.sqrt(torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
    "                                    )\n",
    "            \n",
    "        # for the target, the sqrt can't be negative nor zero so just take it directly.\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "        \n",
    "        # this is flattened for the input expected by mean square error function:\n",
    "        # (N, S, S, 4) -> (N*S*S, 4).\n",
    "        # end_dim = -2 flattens everything that is not the 4.\n",
    "        # the mse here is applied on the entirety of [x, y, sqrt(w), sqrt(h)],\n",
    "        # so by passing the tensor here, for each cell of each example the mse is\n",
    "        # done for their respective x, y, sqrt(w) and sqrt(h) accordingly.\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n",
    "                                          torch.flatten(box_targets, end_dim=-2),\n",
    "                           )\n",
    "\n",
    "        # =============== #\n",
    "        # FOR OBJECT LOSS #\n",
    "        # =============== #\n",
    "        # line 3 of the equations\n",
    "        \n",
    "        # the slicing of 1 term is made to keep the dimensions of the tensor.\n",
    "        # C contains de object probability (0 or 1) for box1, while C+5 contains\n",
    "        # the object probability for box2. \n",
    "        # This line will leave only the box that is responsible for the predictions.\n",
    "        pred_box = (        best_box * predictions[..., (self.C+5):(self.C+6)] \n",
    "                    + (1 - best_box) * predictions[..., (self.C):(self.C+1)]\n",
    "                   )\n",
    "        # flatten is again used to get the expected input by mse, which is:\n",
    "        # (N*S*S, 1). The exists_box is the identity Iobj from before.\n",
    "        # Object loss is just the mean square error of the responsible box's\n",
    "        # \"objectness\" IF the box exists.\n",
    "        object_loss = self.mse(torch.flatten(exists_box * pred_box),\n",
    "                               torch.flatten(exists_box * target[..., self.C:(self.C+1)]),\n",
    "                              )\n",
    "        # ================== #\n",
    "        # FOR NO OBJECT LOSS #\n",
    "        # ================== #\n",
    "        # line 4 of the equations\n",
    "        \n",
    "        # I have no idea why this flatten is made with start_dim = 1 but the previous one wasn't.\n",
    "        # something to do with (N, S, S, 1) -> (N, S*S).\n",
    "        # this line does the mse for box1.\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:(self.C+1)], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:(self.C+1)], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., (self.C+5):(self.C+6)], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:(self.C+1)], start_dim=1)\n",
    "        )\n",
    "        # ============== #\n",
    "        # FOR CLASS LOSS #\n",
    "        # ============== #\n",
    "        # line 5 of the equations\n",
    "        \n",
    "        # class loss uses the class probability for each class, so the tensor\n",
    "        # goes from 0 to C-1. That's why we use :C. \n",
    "        # end_dim = -2 is used for:\n",
    "        # (N, S, S, C) -> (N*S*S, C)\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        #| ========||#\n",
    "        # FINAL LOSS #\n",
    "        #||========|_#\n",
    "        \n",
    "        loss = (self.lambda_coord * box_loss # first two rows of loss in paper \n",
    "                + object_loss # third row. Notice that there is no lambda      \n",
    "                + self.lambda_no_obj * no_object_loss # fourth row             \n",
    "                + class_loss # fifth row. No lambda either                     \n",
    "                )\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f15933-7d62-43fa-8252-b15d37e39185",
   "metadata": {},
   "source": [
    "The dataset is organized in two folders: an images folder and a labels folder. \n",
    "\n",
    "The images are standard .jpg files with three channels BGR. Take care to not mix different representations, like thermal images with standard images, train the network with one or the other.\n",
    "\n",
    "The labels consist of .txt files that contains the data for each box present in the image: class label of the object the boss represents (int), box's horizontal relative to the whole image (float), box's vertical relative to the whole image (float), box's width relative to the whole image (float) and box's height relative to the whole image (float).\n",
    "\n",
    "For example:\n",
    "\n",
    "| | | | | |\n",
    "|-|-|-|-|-|\n",
    "| 11   | 0.341926346090654 | 0.611  | 0.4164305949008499  | 0.262 |\n",
    "| 14   | 0.509915014164306 | 0.51   | 0.9745042492917847  | 0.972 |\n",
    "\n",
    "Two .csv files are used to load the files' locations into the program: train.csv and test.csv. Those files consist of two columns, the first with the name of the image and the second with the name of the label file associated with that image. Do note that only the file names should be there, not their full path.\n",
    "\n",
    "For example:\n",
    "\n",
    "| | | \n",
    "|-|-|\n",
    "| FLIR0045.jpg | FLIR0045.txt  |\n",
    "| FLIR0046.jpg | FLIR0046.txt  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd9980c-df62-4168-909a-14bab89a61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset\n",
    "This class loads ONE example that will be used by an efficient data pipeline\n",
    "to batch everything later.\n",
    "'''\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__ (self, csv_file, img_dir, label_dir, split_size, num_boxes, num_classes, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.C = num_classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # the second colum is where the text files are\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                # the class is an integer, but x, y, w and h aren't.\n",
    "                # so we compare float(x) with int(float(x)) to catch\n",
    "                # if the variable being read is a class label or\n",
    "                # an actual float.\n",
    "                # what is being read is a string in the first place,\n",
    "                # that's why it has to be typecast fo float or int.\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if\n",
    "                        float(x) != int(float(x)) else \n",
    "                    int(float(x)) for\n",
    "                        x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "                \n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "                \n",
    "        # the first column is where the image files are\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path) # we are using PIL to open\n",
    "        # the images are converted to a tensor for the sake of doing\n",
    "        #  transformations with Pytorch if needed.\n",
    "        boxes = torch.tensor(boxes)\n",
    "        \n",
    "        # transform is used if we are doing data augmentation\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "        # the annotated images will only have ONE bounding box per cell, so\n",
    "        # only C + 5 [0:C-1 (class probabilities), objectness, x, y, w, h].\n",
    "        # The *self.B is only there to match the size of the tensor in other\n",
    "        # parts of the code, they don't contain any data for now.\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "\n",
    "        # we need to convert everything to fit this label matrix. The\n",
    "        # bounding boxes in the read images are relative to the entire\n",
    "        # image, but YOLO needs the boxes relative to cells, so a\n",
    "        # a conversion is necessary.\n",
    "        for box in boxes:\n",
    "            # after the transformation, the boxes are converted\n",
    "            # to list again.\n",
    "            class_label, x, y, width, height = box.tolist()       \n",
    "            # this is to make absolute sure the class_label IS an int\n",
    "            class_label = int(class_label)\n",
    "            # we need to see which cell row and cell column each box belongs to.\n",
    "            # to convert this, we multiply the values by the number of cells S to\n",
    "            # escale it appropriately, then by taking the intenger of that the \n",
    "            # number is rounded down to the row i for y and column j for x that\n",
    "            # the box belongs to.\n",
    "            i = int(self.S * y) # row it belongs to\n",
    "            j = int(self.S * x) # column it belongs to            \n",
    "            # the x and y relative to the cell are obtained by subtracting the\n",
    "            # column for x and row for y from the x and y values that got escaled \n",
    "            # by S. In doing so, we only get the decimal part between 0 and 1 \n",
    "            # that indicates where in the cell the centerpoints are.\n",
    "            x_cell = self.S * x - j\n",
    "            y_cell = self.S * y - i\n",
    "            # the original width and height values were relative to the entire\n",
    "            # image, so to get them relative to the cell we just multiply them\n",
    "            # by the number of cells S we split it by.\n",
    "            width_cell  = width*self.S\n",
    "            height_cell = height*self.S\n",
    "            # the objectness is used to determine if there is an object\n",
    "            # in [i, j]. If there is currently no object there (0), we\n",
    "            # are going to use it.\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "                # the boox coordinates are now converted to tensor\n",
    "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
    "                # then added to their position (C+1:C+5) in the label_matrix, this is after\n",
    "                # the class probabibilites and the objectness.\n",
    "                label_matrix[i, j, (self.C+1):(self.C+5)] = box_coordinates\n",
    "                # this sets the index of the class label to 1, to signify\n",
    "                # what class the box represents.\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "                \n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1198d71-2438-4050-9963-6ed4a3995e52",
   "metadata": {},
   "source": [
    " <center><img src='fig/nonmax-1.png' width=\"600\"><center>\n",
    "\n",
    "Every bounding box has a probability score between 0 and 1 for each class independently that corresponds to how likely it is that there is an object of that class in that bounding box. Non-max suppression means that we choose the box with the highest probability score.\n",
    "\n",
    "The intersection of union is used between this box that has the highest probability score and other overlapping boxes. If the IOU returns a value that is higher than a certain threshold, those other overlapping boxes are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3778440a-d796-47aa-ac13-44fade9b69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "non max supression\n",
    "'''\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes, cleaning up overlapping bounding boxes.\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # bboxes = [[class_label, class_probability, x, y, w, h]]\n",
    "    # the bboxes parameter has to be a list for this to work.\n",
    "    assert type(bboxes) == list\n",
    "    \n",
    "    # this only keeps the boxes that have probability above a given threshold.\n",
    "    # a low probability of containing that class' object is immediately pruned.\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    # this sorts the bounding boxes with the highest probability at the beginning.\n",
    "    # we want to choose the box with the highest probability first.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    # empty list that will store the bounding boxes after the nms.\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        # this pops the box with the highest score\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        \n",
    "        # this updates the list of the bounding boxes\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            # if the class of the box is not the same as the class of the\n",
    "            # chosen box, we don't want to compare them. So just for that\n",
    "            # condition, we want to keep that bounding box to be tested\n",
    "            # with its respective class later.\n",
    "            if box[0] != chosen_box[0]\n",
    "            # if they are the same class, we want to compare them.\n",
    "            or intersection_over_union(\n",
    "                # 2: because we are only passing the [x, y, w, h] params\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "                ) < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd4dcb-59f9-4baf-bc8d-7a51200f1bd1",
   "metadata": {},
   "source": [
    "<center> <img src='fig/mAP.png' width=\"600\"> <center>\n",
    "    \n",
    "The mean average precision is the most common metric used in deep learning to evaluate object detection models.\n",
    "    \n",
    "<center> <img src='fig/pre_rec.png' width=\"600\"> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd43388-6741-4ad8-8ee3-2509067f02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mean average precision\n",
    "'''\n",
    "\n",
    "def mean_average_precision(pred_boxes, true_boxes, num_classes, iou_threshold=0.5, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c6acc24-745a-423c-b245-e3a8ce1c6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "box utils. Those functions are responsible for converting the bounding boxes.\n",
    "'''\n",
    "\n",
    "def cellboxes_to_boxes(out, S, B, C):\n",
    "    '''\n",
    "    converts the bounding boxes relative to the cell to be relative to the entire image.\n",
    "    Parameters:\n",
    "        out (list): list with the boxes relative to their cell.\n",
    "        S (int): the dimension of the grid used for the cells.\n",
    "    Returns: \n",
    "        list: list with the x, y, w and h relative to the entire image.\n",
    "    '''\n",
    "    converted_pred = convert_cellboxes(out, S=S, B=B, C=C).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def convert_cellboxes(predictions, S, B, C):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + B * 5)\n",
    "    bboxes1 = predictions[..., (C+1):(C+5)]\n",
    "    bboxes2 = predictions[..., (C+6):(C+10)]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., (C+5)].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., (C+5)]).unsqueeze(-1)\n",
    "                                \n",
    "    converted_preds = torch.cat(\n",
    "                                (predicted_class,\n",
    "                                 best_confidence,\n",
    "                                 converted_bboxes\n",
    "                                ),\n",
    "                                dim=-1\n",
    "                               )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def get_bboxes(loader,\n",
    "               model,\n",
    "               iou_threshold,\n",
    "               threshold,\n",
    "               split_size,\n",
    "               num_boxes,\n",
    "               num_classes,\n",
    "               pred_format=\"cells\",\n",
    "               box_format=\"midpoint\",\n",
    "               device=\"cuda\",\n",
    "              ):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(out = labels, S = split_size, B = num_boxes, C = num_classes)\n",
    "        bboxes = cellboxes_to_boxes(out = predictions, S = split_size, B = num_boxes, C = num_classes)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes = bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "\n",
    "            #if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "class early_stopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    a certain amount of epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience, min_delta, counter = 0, best_loss = None, early_stop = False):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = counter\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "    def params_getter(self):\n",
    "        return self.patience, self.min_delta, self.counter, self.best_loss, self.early_stop\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecbe6e6-b3ce-4ec2-9139-2a617fddae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot functions\n",
    "'''\n",
    "# this is the function that I need to change to be able to plot\n",
    "# the class names\n",
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle potch\n",
    "    for box in boxes:\n",
    "        # the class name is obtained using the class index\n",
    "        class_name = CLASSES[int(box[0])]\n",
    "        class_prob = \"{:.3f}\".format(box[1])\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(upper_left_x*width, upper_left_y*height, class_name, color=\"r\")\n",
    "        ax.text(upper_left_x*width, upper_left_y*height - 20, class_prob, color=\"r\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def loss_graph(show = False):\n",
    "    fig, ax = plt.subplots(facecolor='white')\n",
    "    epochs = [i for i in range(0, len(val_loss))]\n",
    "    plt.plot(epochs, val_loss, 'b--', label = 'Validation loss')\n",
    "    plt.plot(epochs, train_loss, 'r--', label = 'Training loss')\n",
    "    plt.axvline(x=loss_checkpoint, color='black', linestyle='--', label = f'Val loss checkpoint (patience = {VAL_LOSS_PATIENCE})')\n",
    "    plt.axvline(x=mAP_checkpoint, color='gray', linestyle='--', label = 'Train mAP checkpoint (mAP >= 0.9)')\n",
    "    ax.set(xlabel='Epochs', ylabel='Loss', title=\"Loss x Epochs\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    fig.savefig(\"Loss x Epochs - Training vs Validation\" + GRAPH_NAME + \".png\")\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def mAP_graph(show = False):\n",
    "    fig, ax = plt.subplots(facecolor='white')\n",
    "    epochs = [i for i in range(0, len(val_mAP))]\n",
    "    plt.plot(epochs, val_mAP, 'b--', label = 'Validation mAP')\n",
    "    plt.plot(epochs, train_mAP, 'r--', label = 'Training mAP')\n",
    "    plt.axvline(x=loss_checkpoint, color='black', linestyle='--', label = f'Val mAP checkpoint (patience = {VAL_LOSS_PATIENCE})')\n",
    "    plt.axvline(x=mAP_checkpoint, color='gray', linestyle='--', label = 'Train mAP checkpoint (mAP >= 0.9)')\n",
    "    ax.set(xlabel='Epochs', ylabel='mAP', title=\"mAP x Epochs\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    fig.savefig(\"mAP x Epochs - Training vs Validation\" + GRAPH_NAME + \".png\")\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4296a10-2967-41a8-a4be-4175fafac650",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "checkpoint functions\n",
    "'''\n",
    "def save_checkpoint(state, filename=\"final.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a667dbc-f6f4-4ec0-9aaa-2235263f5585",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train\n",
    "'''\n",
    "# to always load the same dataset\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "#LEARNING_RATE = 2e-5\n",
    "#LEARNING_RATE = 0.0013\n",
    "LEARNING_RATE = 1e-4 # previouslu 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "# dropout and weight decay are both 0 here for now, but\n",
    "# we need to change them to something else if we intend \n",
    "# to actually train the model, use data augmentation and\n",
    "# and whatnot.\n",
    "# We put weight_decay to 0 to OVERFIT the image on purpose,\n",
    "# just to check if the model IS working as intended.\n",
    "#WEIGHT_DECAY = 0\n",
    "WEIGHT_DECAY = 0.0005\n",
    "# WIDTH and HEIGHT must be multiples of 7?\n",
    "WIDTH = 448 # 640\n",
    "HEIGHT = 448 # 480 \n",
    "EPOCHS = 100\n",
    "NUM_WORKERS = 0 # I'd use 2 if the computer allowed.\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"aug_0001lr.pth.tar\"\n",
    "SAVE_MODEL_FILE = \"aug_0001lr\"\n",
    "#LOAD_MODEL_FILE = \"640x480_save.pth.tar\"\n",
    "#SAVE_MODEL_FILE = \"640x480\"\n",
    "GRAPH_NAME = \" for aug_0001lr input\"\n",
    "IMG_DIR = \"./Data/aug_images/\"\n",
    "LABEL_DIR = \"./Data/aug_labels/\"\n",
    "NUM_BOXES = 2\n",
    "SPLIT_SIZE = 7\n",
    "NUM_CLASSES = 4\n",
    "VAL_LOSS_PATIENCE = 50 # 30 was too low, maybe 50 next time?\n",
    "VAL_LOSS_DELTA = 100 # 20 was STILL too low\n",
    "# reading CSV file for class names\n",
    "CLASSES = pd.read_csv(\"names.csv\", names = [\"class\"])\n",
    "CLASSES = CLASSES[\"class\"].tolist()\n",
    "AUG_INPLACE = False\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        # the transform only operates on the images, we don't\n",
    "        # want to transform the bounding boxes. They are already\n",
    "        # relative to the image in ratios, regardless of its size.\n",
    "        # IF the transforms used were also for data augmentation,\n",
    "        # then we should do t(img, bboxes). This will be implemented\n",
    "        # in the future perhaps.\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "# the input images will be resized to 448x448\n",
    "if AUG_INPLACE == \"False\":\n",
    "    scale = 1.1\n",
    "    # p is the probability of applying a given transform\n",
    "    # longestMaxSize keeps the aspect ratio of the image\n",
    "    # PadIfNeeded will be used pad the empty space\n",
    "    # 1.1 scale is used to minimize the padded area\n",
    "    # RandomCrop is applied on top of that to make the resulting image be\n",
    "    # within the input size of the network, but each epoch will have variations.\n",
    "    train_transforms = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=int(max(WIDTH, HEIGHT) * scale)), \n",
    "            A.PadIfNeeded(\n",
    "                min_height=int(HEIGHT * scale),\n",
    "                min_width=int(WIDTH * scale),\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "            ),\n",
    "            A.RandomCrop(width=WIDTH, height=HEIGHT),\n",
    "            A.Multiply((0.7, 1.3), p=0.5),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.ShiftScaleRotate(\n",
    "                        rotate_limit=10, p=0.5, border_mode=cv2.BORDER_REFLECT_101\n",
    "                    ),\n",
    "                    A.Perspective (scale=(0, 0.1), p=0.5),\n",
    "                ],\n",
    "                p=1.0,\n",
    "            ),\n",
    "            A.Cutout(num_holes=4, max_h_size=8, max_w_size=8, fill_value=0, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.8, label_fields=[],),\n",
    "    )\n",
    "    test_transforms = A.Compose(\n",
    "            [\n",
    "                A.LongestMaxSize(max_size= max(IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "                A.PadIfNeeded(\n",
    "                    min_height=IMAGE_HEIGHT, min_width=IMAGE_WIDTH, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),)\n",
    "else:\n",
    "    train_transforms = Compose([transforms.Resize((WIDTH, HEIGHT)), transforms.ToTensor(),])\n",
    "    test_transforms  = Compose([transforms.Resize((WIDTH, HEIGHT)), transforms.ToTensor(),])\n",
    "       \n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    # leave true is for the progress bar\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    mean_loss = sum(mean_loss)/len(mean_loss)\n",
    "    print(f\"Training mean loss was {mean_loss}\")\n",
    "    return mean_loss\n",
    "\n",
    "def validation_fn(valid_loader, model, loss_fn):\n",
    "    # Settings\n",
    "    model.eval()\n",
    "    loop = tqdm(valid_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    # Test validation data\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            mean_loss.append(loss.item())\n",
    "            \n",
    "            # update the progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    mean_loss = sum(mean_loss)/len(mean_loss)\n",
    "    print(f\"Validation mean loss was {mean_loss}\")\n",
    "    model.train()\n",
    "    return mean_loss\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_mAP = []\n",
    "val_mAP = []\n",
    "loss_checkpoint = 0\n",
    "mAP_checkpoint = 0\n",
    "log_lines = \"\"\n",
    "early_stopping_params = [[VAL_LOSS_PATIENCE, VAL_LOSS_DELTA, 0, 0, False, False, loss_checkpoint, mAP_checkpoint]]\n",
    "early_stopping_params = pd.DataFrame(early_stopping_params, columns=[\"patience\", \"min_delta\", \"counter\", \"best_loss\", \"early_stop\", \"train_precision_stopped\", \"loss_checkpoint\", \"mAP_checkpoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ce5c76-e266-45be-b3d9-ec990bff2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = datetime.now(pytz.timezone('Brazil/East'))\n",
    "    # Flags to control saving the model for early stopping\n",
    "    loss_stopped = False\n",
    "    train_precision_stopped = False\n",
    "    global loss_checkpoint\n",
    "    global mAP_checkpoint\n",
    "    global early_stopping_params\n",
    "    global train_loss\n",
    "    global val_loss\n",
    "    global train_mAP\n",
    "    global val_mAP\n",
    "    global log_lines\n",
    "    \n",
    "    log_file = open(\"timelog \" + SAVE_MODEL_FILE + \".txt\", \"w\")\n",
    "    log_lines = f'Training began at: {start_time.strftime(\"%m/%d/%Y, %H:%M:%S\")}\\n'\n",
    "    log_file.writelines(log_lines)\n",
    "    log_file.close()\n",
    "\n",
    "    model = Yolov1(split_size=SPLIT_SIZE, num_boxes=NUM_BOXES, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    #RMSprop\n",
    "    #optimizer = optim.SGD(\n",
    "    #    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    #)\n",
    "    loss_fn = YoloLoss(split_size = SPLIT_SIZE, num_boxes = NUM_BOXES, num_classes = NUM_CLASSES)\n",
    "    \n",
    "    loaded_epoch = 0\n",
    "    if LOAD_MODEL:\n",
    "        checkpoint = torch.load(LOAD_MODEL_FILE)\n",
    "        load_checkpoint(checkpoint, model, optimizer)\n",
    "        loaded_epoch = checkpoint['epoch']\n",
    "        loss_fn = checkpoint['loss']\n",
    "        early_stopping_params = pd.read_csv('early_stopping_params_' + SAVE_MODEL_FILE + '.csv')\n",
    "        loss_stopping = early_stopping(patience   = early_stopping_params[\"patience\"].item(), \n",
    "                                       min_delta  = early_stopping_params[\"min_delta\"].item(), \n",
    "                                       counter    = early_stopping_params[\"counter\"].item(),\n",
    "                                       best_loss  = early_stopping_params[\"best_loss\"].item(),\n",
    "                                       early_stop = early_stopping_params[\"early_stop\"].item())\n",
    "        loss_stopped = early_stopping_params[\"early_stop\"].item()\n",
    "        loss_checkpoint = early_stopping_params[\"loss_checkpoint\"].item()\n",
    "        mAP_checkpoint = early_stopping_params[\"mAP_checkpoint\"].item()\n",
    "        train_precision_stopped = early_stopping_params[\"train_precision_stopped\"].item()\n",
    "        \n",
    "        train_loss = pd.read_csv('train_loss_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        val_loss = pd.read_csv('val_loss_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        train_mAP = pd.read_csv('train_mAP_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        val_mAP = pd.read_csv('val_mAP_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        \n",
    "    else:\n",
    "        loss_stopping = early_stopping(patience = VAL_LOSS_PATIENCE, min_delta = VAL_LOSS_DELTA)\n",
    "\n",
    "\n",
    "    train_dataset = VOCDataset(\n",
    "        \"./Data/aug_train.csv\",\n",
    "        transform=train_transforms,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "        split_size = SPLIT_SIZE, \n",
    "        num_boxes = NUM_BOXES, \n",
    "        num_classes = NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    validation_dataset = VOCDataset(\n",
    "        \"./Data/aug_validation.csv\", \n",
    "        transform=test_transforms, \n",
    "        img_dir=IMG_DIR, \n",
    "        label_dir=LABEL_DIR, \n",
    "        split_size = SPLIT_SIZE,\n",
    "        num_boxes = NUM_BOXES,\n",
    "        num_classes = NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        # if there is a batch and there are only two examples,\n",
    "        # we don't want to do an update step on that, so\n",
    "        # drop_last SHOULD be True. Check why I can't make it so.\n",
    "        drop_last=True, # if True gets a division by 0\n",
    "    )\n",
    "\n",
    "    validation_loader = DataLoader(\n",
    "        dataset=validation_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    #summary(model, (3, WIDTH, HEIGHT))\n",
    "    #return\n",
    "    for epoch in range(loaded_epoch, EPOCHS):\n",
    "        '''\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(DEVICE)            \n",
    "            for idx in range(8):\n",
    "              bboxes = cellboxes_to_boxes(out = model(x), S = SPLIT_SIZE, B = NUM_BOXES, C = NUM_CLASSES)\n",
    "              bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "              plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "            #import sys\n",
    "            #sys.exit()\n",
    "        '''\n",
    "        print(\"_\"*50)\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        \n",
    "        print(\"\\nTrain set: \")\n",
    "        pred_boxes, target_boxes = get_bboxes(loader        = train_loader, \n",
    "                                              model         = model, \n",
    "                                              iou_threshold = 0.5, \n",
    "                                              threshold     = 0.4, \n",
    "                                              split_size    = SPLIT_SIZE, \n",
    "                                              num_boxes     = NUM_BOXES, \n",
    "                                              num_classes   = NUM_CLASSES,\n",
    "                                              box_format    = \"midpoint\",\n",
    "                                              device        = DEVICE\n",
    "                                             )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes    = pred_boxes,\n",
    "                                               true_boxes    = target_boxes,\n",
    "                                               iou_threshold = 0.5,\n",
    "                                               box_format    = \"midpoint\",\n",
    "                                               num_classes   = NUM_CLASSES\n",
    "                                              )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "        train_mAP.append(float(mean_avg_prec))\n",
    "        if(epoch%10 == 0):\n",
    "            checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \".pth.tar\")\n",
    "            \n",
    "        if ((mean_avg_prec >= 0.90) and (train_precision_stopped == False)):\n",
    "            mAP_checkpoint = epoch\n",
    "            early_stopping_params[\"mAP_checkpoint\"] = mAP_checkpoint\n",
    "            checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            }\n",
    "            print(f\"Saving checkpoint due to mAP reaching threshold 0.90 at epoch {mAP_checkpoint}.\")\n",
    "            print(epoch)\n",
    "            save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \"train_mAP_stopped.pth.tar\")\n",
    "            train_precision_stopped = True\n",
    "            early_stopping_params[\"train_precision_stopped\"] = True\n",
    "            early_stopping_params.to_csv('early_stopping_params_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "            \n",
    "            \n",
    "        train_loss.append(train_fn(train_loader, model, optimizer, loss_fn))\n",
    "        \n",
    "        print(\"\\nValidation set: \")\n",
    "        pred_boxes, target_boxes = get_bboxes(loader        = validation_loader, \n",
    "                                              model         = model, \n",
    "                                              iou_threshold = 0.5, \n",
    "                                              threshold     = 0.4, \n",
    "                                              split_size    = SPLIT_SIZE, \n",
    "                                              num_boxes     = NUM_BOXES, \n",
    "                                              num_classes   = NUM_CLASSES,\n",
    "                                              box_format    = \"midpoint\",\n",
    "                                              device        = DEVICE\n",
    "                                             )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes    = pred_boxes,\n",
    "                                               true_boxes    = target_boxes,\n",
    "                                               iou_threshold = 0.5,\n",
    "                                               box_format    = \"midpoint\",\n",
    "                                               num_classes   = NUM_CLASSES\n",
    "                                              )\n",
    "        \n",
    "        print(f\"Validation mAP: {mean_avg_prec}\")\n",
    "        val_mAP.append(float(mean_avg_prec))\n",
    "        val_loss.append(validation_fn(validation_loader, model, loss_fn))\n",
    "        \n",
    "        if loss_stopped == False:\n",
    "            loss_stopping(val_loss[-1])\n",
    "            (early_stopping_params[\"patience\"], \n",
    "            early_stopping_params[\"min_delta\"], \n",
    "            early_stopping_params[\"counter\"],\n",
    "            early_stopping_params[\"best_loss\"],\n",
    "            early_stopping_params[\"early_stop\"]) = loss_stopping.params_getter()\n",
    "            early_stopping_params.to_csv('early_stopping_params_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "            if (loss_stopping.early_stop == True):\n",
    "                loss_checkpoint = epoch\n",
    "                early_stopping_params[\"loss_checkpoint\"] = loss_checkpoint\n",
    "                early_stopping_params.to_csv('early_stopping_params_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "                checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                'loss': loss_fn,\n",
    "                }\n",
    "                print(f\"Saving checkpoint due to validation loss at epoch {loss_checkpoint}.\")\n",
    "                print(epoch)\n",
    "                save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \"loss_stopped.pth.tar\")\n",
    "                loss_stopped = True\n",
    "            \n",
    "            #break\n",
    "        dataframe = pd.DataFrame(train_loss)\n",
    "        dataframe.to_csv('train_loss_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        dataframe = pd.DataFrame(val_loss) \n",
    "        dataframe.to_csv('val_loss_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        dataframe = pd.DataFrame(train_mAP)\n",
    "        dataframe.to_csv('train_mAP_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        dataframe = pd.DataFrame(val_mAP) \n",
    "        dataframe.to_csv('val_mAP_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        loss_graph()\n",
    "        mAP_graph()\n",
    "        \n",
    "    dataframe = pd.DataFrame(train_loss)\n",
    "    dataframe.to_csv('train_loss_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "    dataframe = pd.DataFrame(val_loss) \n",
    "    dataframe.to_csv('val_loss_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "    dataframe = pd.DataFrame(train_mAP)\n",
    "    dataframe.to_csv('train_mAP_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "    dataframe = pd.DataFrame(val_mAP) \n",
    "    dataframe.to_csv('val_mAP_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "    loss_graph(show = True)\n",
    "    mAP_graph(show = True)\n",
    "    \n",
    "    save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \".pth.tar\")\n",
    "    end_time = datetime.now(pytz.timezone('Brazil/East'))\n",
    "    \n",
    "    log_file = open(\"timelog \" + SAVE_MODEL_FILE + \".txt\", \"w\")\n",
    "    log_lines += f'Training ended at: {end_time.strftime(\"%m/%d/%Y, %H:%M:%S\")}\\n'\n",
    "    log_lines += f'Training time: {(end_time - start_time)}\\n'\n",
    "    log_file.writelines(log_lines)\n",
    "    log_file.close()\n",
    "    \n",
    "    print(\"Training began at: \", start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "    print(\"Training ended at: \", end_time.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "    print(\"Training time: \", (end_time - start_time))\n",
    "    textfile.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a0e55-e798-426e-b571-2ec823f5136f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Epoch: 0\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.0\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798b8bff7e634bd88fb7992e7d83359d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a0a172c-65fb-41dc-bdcf-699b47fdb972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig, ax = plt.subplots(facecolor='white')\n",
    "#epochs = [i for i in range(0, len(val_loss))]\n",
    "#plt.plot(epochs, val_loss, 'b--', label = 'Validation loss')\n",
    "#plt.ylim(bottom = 0, top = 1600)\n",
    "#plt.plot(epochs, train_loss, 'r--', label = 'Training loss')\n",
    "#plt.axvline(x=loss_checkpoint, color='black', linestyle='--', label = f'Val loss checkpoint (patience = {VAL_LOSS_PATIENCE})')\n",
    "#plt.axvline(x=mAP_checkpoint, color='gray', linestyle='--', label = 'Train mAP checkpoint (mAP >= 0.9)')\n",
    "#ax.set(xlabel='Epochs', ylabel='Loss', title=\"Loss x Epochs\")\n",
    "#ax.grid()\n",
    "#plt.legend()\n",
    "#fig.savefig(\"Loss x Epochs - Training vs Validation clipped\" + GRAPH_NAME + \".png\")\n",
    "#plt.show()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaba285-8eac-438b-9fe3-ef42bb4213a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
