{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8cc60a-c486-4a25-8486-09c36e3ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os # dataset\n",
    "import pandas as pd # dataset\n",
    "from PIL import Image # dataset\n",
    "import torchvision.transforms as transforms # train\n",
    "import torch.optim as optim # train\n",
    "import torchvision.transforms.functional as FT # train\n",
    "#from tqdm import tqdm # train\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torch.utils.data import DataLoader # train\n",
    "import numpy as np # box utils\n",
    "import matplotlib.pyplot as plt # box utils\n",
    "import matplotlib.patches as patches # box utils\n",
    "from collections import Counter # box utils\n",
    "\n",
    "import albumentations as A # train\n",
    "from albumentations.pytorch import ToTensorV2 # train\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary # main\n",
    "import matplotlib.pyplot as plt # plot functions (loss and mAP curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c984a-4a21-4107-b047-798355d55c94",
   "metadata": {},
   "source": [
    "<center><img src='fig/YOLO-v1-network.png'><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5158d96-ea50-42ea-80fd-645970d36cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model\n",
    "'''\n",
    "# nout = [(nin + 2*p - k)/s] + 1\n",
    "# nin = number of input features\n",
    "# nout = number of output features\n",
    "# k = convolution kernel size\n",
    "# p = convolution padding size\n",
    "# s = convolution stride size\n",
    "architecture_config = [\n",
    "    # Tuple: (kernel_size, num_filters, stride, padding)\n",
    "    (7, 64, 2, 3), # original size\n",
    "    #(7, 64, 3, (18, 98)), # this was done to keep the 224x224 ratio after this layer\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    # List: two tuples and then last integer represents number of repeats\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "# CNN block will be a conv layer, the batch norm and then a relu\n",
    "class CNNBlock(nn.Module):\n",
    "    # our **kwargs will be split_size, num_boxes, num_classes\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        # we are going to use batchnorm, so bias = False\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "# 3 channels because of the bgr images\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        # the model in the beginning of this block\n",
    "        self.architecture = architecture_config\n",
    "        # the number of channels of the image\n",
    "        self.in_channels = in_channels\n",
    "        # the conv layers mirror what Redmon did in his darknet\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        # fully connected\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        # start_dim = 1 to not flatten the number of examples\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "    \n",
    "    def _create_conv_layers(self, architecture):\n",
    "        # all layers will be added in this empty list, created from the architecture\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        # this will go over every tuple/list in the architecture\n",
    "        for x in architecture:\n",
    "            # for a tuple, we add a CNNBlock\n",
    "            if type(x) == tuple:\n",
    "                # x arguments for a tuple being (kernel_size, num_filters, stride, padding)\n",
    "                # x[0] = kernel_size\n",
    "                # x[1] = num_filters\n",
    "                # x[2] = stride\n",
    "                # x[3] = padding\n",
    "                layers += [CNNBlock(in_channels=in_channels,\n",
    "                                  out_channels=x[1],\n",
    "                                  kernel_size=x[0],\n",
    "                                  stride=x[2],\n",
    "                                  padding=x[3],\n",
    "                                 )\n",
    "                         ]\n",
    "                # in_channels has to change for the next element\n",
    "                # to the format outputted by this tuple\n",
    "                in_channels = x[1]\n",
    "                #print(x[1])\n",
    "                \n",
    "            # for the \"M\" in the architecture a maxpool layer is added\n",
    "            elif type(x) == str:\n",
    "                # kernel_size and stride are both 2x2 and a single argument of 2\n",
    "                # is understood as (2, 2)\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            \n",
    "            # the lists in the architecture all consist of two tuples and\n",
    "            # a last integer that represents number of repeats\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0] # tuple\n",
    "                conv2 = x[1] # tuple\n",
    "                num_repeats = x[2] # integer\n",
    "\n",
    "                # the indexes of the tuple elements are the same as the singular tuples:\n",
    "                # conv[0] = kernel_size\n",
    "                # conv[1] = num_filters\n",
    "                # conv[2] = stride\n",
    "                # conv[3] = padding\n",
    "                for _ in range(num_repeats):\n",
    "                    # first we add the first tuple of the list\n",
    "                    layers += [CNNBlock(in_channels=in_channels,\n",
    "                                        out_channels=conv1[1],\n",
    "                                        kernel_size=conv1[0],\n",
    "                                        stride=conv1[2],\n",
    "                                        padding=conv1[3],\n",
    "                                        )\n",
    "                              ]\n",
    "                    # then the second tuple of the list.\n",
    "                    # the in_channels is now the output from the\n",
    "                    # FIRST conv layer, so conv1[1]\n",
    "                    layers += [CNNBlock(in_channels=conv1[1],\n",
    "                                        out_channels=conv2[1],\n",
    "                                        kernel_size=conv2[0],\n",
    "                                        stride=conv2[2],\n",
    "                                        padding=conv2[3],\n",
    "                                        )\n",
    "                              ]\n",
    "                    # we now change in_channels itself to the num_filters\n",
    "                    # of the second tuple, because it will be the input of\n",
    "                    # the first tuple the next time the loop iterates\n",
    "                    in_channels = conv2[1]\n",
    "                    \n",
    "        # we do * to unpack the list, which then will be converted to a nn.Sequential\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    # the linear fully connected layer with 4096 entries before the final output\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        # this will be reshaped later in the loss function to\n",
    "        # not be linear anymore\n",
    "        return nn.Sequential(nn.Flatten(),\n",
    "                             nn.Linear(1024 * S * S, 4096),\n",
    "                             nn.Dropout(0.0),\n",
    "                             nn.LeakyReLU(0.1),\n",
    "                             nn.Linear(4096, S * S * (C + B * 5))\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec387260-7757-4956-be04-6752d5e576ed",
   "metadata": {},
   "source": [
    "<center><img src='fig/iou.png' width=\"600\"><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e091407-d14e-4a47-a441-aff078a9c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IOU\n",
    "'''\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union.\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes are (x, y, w, h) or (x1, y1, x2, y2)\n",
    "    \n",
    "    Returns:\n",
    "        tensor: intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    # boxes_labels shape is (N, 4)\n",
    "    # ... means all the dimensions before the last\n",
    "    # we do this [..., 0:1] thing to slice the tensor\n",
    "    # and keep the N dimensions, so we get (N, 1)\n",
    "    if box_format == \"midpoint\":\n",
    "        # (x, y, w, h)\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "        \n",
    "    if box_format == \"corners\":\n",
    "        #(x1, x2, y1, y2)\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]    \n",
    "    \n",
    "    # if it's midpoint, we need to do calculations first.\n",
    "    # midpoint gives x and y for the center of the box, then\n",
    "    # also passes the arguments h and w for its height and\n",
    "    # width. Since we are at the center of the object, half\n",
    "    # of width and height is used to find the top, left, right\n",
    "    # and bottoms.\n",
    "    # Reminder that up left corner of an image is x, y = 0.\n",
    "    # top = y - h/2\n",
    "    # bottom = y + h/2\n",
    "    # left = x - w/2\n",
    "    # right = x + w/2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    \n",
    "    # if they don't intersect, at least one will be 0.\n",
    "    # .clamp(0) is for such edge case, because if they\n",
    "    # don't intersect, the intersection SHOULD be 0.\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y1 - box1_y2))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y1 - box2_y2))\n",
    "    \n",
    "    # the union is just the sum of the area of each box minus\n",
    "    # their intersection.\n",
    "    union = (box1_area + box2_area) - intersection\n",
    "    \n",
    "    # that 1e-6 is added for numerical stability\n",
    "    return intersection / (union + 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb97a3-97bb-4523-a08b-538107b84f72",
   "metadata": {},
   "source": [
    "<center><img src='fig/loss.png'><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2baee61-819f-4818-9beb-c9eec8c868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loss\n",
    "'''\n",
    "class YoloLoss(nn.Module):\n",
    "    # our kwargs will be split_size, num_boxes, num_classes, no_obj and coord\n",
    "    def __init__(self, split_size, num_boxes, num_classes, no_obj = 0.5, coord = 5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        # mean square error is what we use for loss\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.C = num_classes\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_no_obj = no_obj\n",
    "        self.lambda_coord = coord\n",
    "        \n",
    "    # Target:\n",
    "    # (S, S, [C] + prob_s + [x, y, w, h]) \n",
    "    # = (S, S, C + 5)\n",
    "    # The target only has one box, while YOLO predicts two boxes per cell\n",
    "    # Prediction:\n",
    "    # (S, S, [C] + prob_s1 + [x1, y1, w1, h1] + prob_s2 + [x2, y2, w2, h2])\n",
    "    # = (S, S, C + 10)\n",
    "    def forward(self, predictions, target):\n",
    "        #-1 keeps the number of examples, \n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "        # (S, S, C + 10)\n",
    "        # the probability score for box1 is at position C, so C+1:C+5 contains the box's [x, y, w, h]\n",
    "        iou_b1 = intersection_over_union(predictions[..., (self.C+1):(self.C+5)], target[..., (self.C+1):(self.C+5)])\n",
    "        # the probability score for box2 is at position C+5, so C+6:C+10 for that box's [x, y, w, h]\n",
    "        iou_b2 = intersection_over_union(predictions[..., (self.C+6):(self.C+10)], target[..., (self.C+1):(self.C+5)])\n",
    "        # torch.unsqueeze adds an additional dimension to the tensor. unsqueze(0) adds that \n",
    "        # before all others, so for example an array of size (5) would become the matrix (1, 5)\n",
    "        # torch.cat concatenates the given sequence of tensors in the given dimension.\n",
    "        # So by first unsqueezing them into (1, <tensor>) before concatenating them in the 0th\n",
    "        # dimension we are ordering the ious sequentially. iou[0] will contain iou_b1 and iou[1]\n",
    "        # will contain iou_b2. You can't concatenate tensors if you don't add that extra dimension\n",
    "        # first.\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "        # torch.max returns the result tuple of two output tensors (max, max_indices).\n",
    "        # what interest us is the best_box, not the max values from iou_maxes. This box\n",
    "        # is the iou responsible for that cell.\n",
    "        iou_maxes, best_box = torch.max(ious, dim=0)\n",
    "        # we want the probability of an object being there, which is stored at index C,\n",
    "        # after the individual class probabilities that are from 0 to C-1. This value is\n",
    "        # 0 or 1 depending if there is an object in that cell.\n",
    "        # This gives the identity function Iobj_i used in the first and fifth lines of the image.\n",
    "        # Unsqueeze is used because when acessing the Cth index, the last dimension disappears. We\n",
    "        # want to add that back.\n",
    "        exists_box = target[..., self.C].unsqueeze(3) # Iobj_i\n",
    "        #print(exists_box)\n",
    "        \n",
    "        # =================== #\n",
    "        # FOR BOX COORDINATES #\n",
    "        # =================== #\n",
    "        # lines 1 and 2 of the equations\n",
    "        \n",
    "        # best box will be 0 if box1 was correct and 1, if box2 was. We use\n",
    "        # this property to zero the tensor that has the dimensions of the wrong\n",
    "        # box, by multiplyer bext_box with [x2, y2, w2, h2] and (1 - best_box) \n",
    "        # with [x1, y1, w1, h1]. \n",
    "        # exists_box multiplies this all so predictions are only considered if THERE\n",
    "        # is a box in the first place from the \"objectness\" property.\n",
    "        box_predictions = exists_box * ((        best_box * predictions[..., (self.C+6):(self.C+10)]\n",
    "                                        + (1 - best_box) * predictions[..., (self.C+1):(self.C+5)]\n",
    "                                       ))\n",
    "        # same exists_box thing with target, but target only has one box.\n",
    "        box_targets = exists_box * target[..., (self.C+1):(self.C+5)]\n",
    "        \n",
    "        # after the previous two lines, box_predictions and box_targets now contain tensors\n",
    "        # of shape (N, S, S, 4), N being the number of input examples. By indexing [..., 2:4]\n",
    "        # we are looking at [w, h] from [x, y, w, h] for the SxS cells of each example.\n",
    "        # torch.sign is used to get the sign of the gradient, then the sqrt is taken from the\n",
    "        # absolute value because the values could be negative. If the result of the sqrt is\n",
    "        # zero, the derivative can go to infinity, so 1e-6 is used for numerical stability.\n",
    "        box_predictions[..., 2:4] = (torch.sign(box_predictions[..., 2:4]) *\n",
    "                                    torch.sqrt(torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
    "                                    )\n",
    "            \n",
    "        # for the target, the sqrt can't be negative nor zero so just take it directly.\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "        \n",
    "        # this is flattened for the input expected by mean square error function:\n",
    "        # (N, S, S, 4) -> (N*S*S, 4).\n",
    "        # end_dim = -2 flattens everything that is not the 4.\n",
    "        # the mse here is applied on the entirety of [x, y, sqrt(w), sqrt(h)],\n",
    "        # so by passing the tensor here, for each cell of each example the mse is\n",
    "        # done for their respective x, y, sqrt(w) and sqrt(h) accordingly.\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n",
    "                                          torch.flatten(box_targets, end_dim=-2),\n",
    "                           )\n",
    "\n",
    "        # =============== #\n",
    "        # FOR OBJECT LOSS #\n",
    "        # =============== #\n",
    "        # line 3 of the equations\n",
    "        \n",
    "        # the slicing of 1 term is made to keep the dimensions of the tensor.\n",
    "        # C contains de object probability (0 or 1) for box1, while C+5 contains\n",
    "        # the object probability for box2. \n",
    "        # This line will leave only the box that is responsible for the predictions.\n",
    "        pred_box = (        best_box * predictions[..., (self.C+5):(self.C+6)] \n",
    "                    + (1 - best_box) * predictions[..., (self.C):(self.C+1)]\n",
    "                   )\n",
    "        # flatten is again used to get the expected input by mse, which is:\n",
    "        # (N*S*S, 1). The exists_box is the identity Iobj from before.\n",
    "        # Object loss is just the mean square error of the responsible box's\n",
    "        # \"objectness\" IF the box exists.\n",
    "        object_loss = self.mse(torch.flatten(exists_box * pred_box),\n",
    "                               torch.flatten(exists_box * target[..., self.C:(self.C+1)]),\n",
    "                              )\n",
    "        # ================== #\n",
    "        # FOR NO OBJECT LOSS #\n",
    "        # ================== #\n",
    "        # line 4 of the equations\n",
    "        \n",
    "        # I have no idea why this flatten is made with start_dim = 1 but the previous one wasn't.\n",
    "        # something to do with (N, S, S, 1) -> (N, S*S).\n",
    "        # this line does the mse for box1.\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:(self.C+1)], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:(self.C+1)], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., (self.C+5):(self.C+6)], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:(self.C+1)], start_dim=1)\n",
    "        )\n",
    "        # ============== #\n",
    "        # FOR CLASS LOSS #\n",
    "        # ============== #\n",
    "        # line 5 of the equations\n",
    "        \n",
    "        # class loss uses the class probability for each class, so the tensor\n",
    "        # goes from 0 to C-1. That's why we use :C. \n",
    "        # end_dim = -2 is used for:\n",
    "        # (N, S, S, C) -> (N*S*S, C)\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        #| ========||#\n",
    "        # FINAL LOSS #\n",
    "        #||========|_#\n",
    "        \n",
    "        loss = (self.lambda_coord * box_loss # first two rows of loss in paper \n",
    "                + object_loss # third row. Notice that there is no lambda      \n",
    "                + self.lambda_no_obj * no_object_loss # fourth row             \n",
    "                + class_loss # fifth row. No lambda either                     \n",
    "                )\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f15933-7d62-43fa-8252-b15d37e39185",
   "metadata": {},
   "source": [
    "The dataset is organized in two folders: an images folder and a labels folder. \n",
    "\n",
    "The images are standard .jpg files with three channels BGR. Take care to not mix different representations, like thermal images with standard images, train the network with one or the other.\n",
    "\n",
    "The labels consist of .txt files that contains the data for each box present in the image: class label of the object the boss represents (int), box's horizontal relative to the whole image (float), box's vertical relative to the whole image (float), box's width relative to the whole image (float) and box's height relative to the whole image (float).\n",
    "\n",
    "For example:\n",
    "\n",
    "| | | | | |\n",
    "|-|-|-|-|-|\n",
    "| 11   | 0.341926346090654 | 0.611  | 0.4164305949008499  | 0.262 |\n",
    "| 14   | 0.509915014164306 | 0.51   | 0.9745042492917847  | 0.972 |\n",
    "\n",
    "Two .csv files are used to load the files' locations into the program: train.csv and test.csv. Those files consist of two columns, the first with the name of the image and the second with the name of the label file associated with that image. Do note that only the file names should be there, not their full path.\n",
    "\n",
    "For example:\n",
    "\n",
    "| | | \n",
    "|-|-|\n",
    "| FLIR0045.jpg | FLIR0045.txt  |\n",
    "| FLIR0046.jpg | FLIR0046.txt  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd9980c-df62-4168-909a-14bab89a61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset\n",
    "This class loads ONE example that will be used by an efficient data pipeline\n",
    "to batch everything later.\n",
    "'''\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__ (self, csv_file, img_dir, label_dir, split_size, num_boxes, num_classes, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.C = num_classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # the second colum is where the text files are\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                # the class is an integer, but x, y, w and h aren't.\n",
    "                # so we compare float(x) with int(float(x)) to catch\n",
    "                # if the variable being read is a class label or\n",
    "                # an actual float.\n",
    "                # what is being read is a string in the first place,\n",
    "                # that's why it has to be typecast fo float or int.\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if\n",
    "                        float(x) != int(float(x)) else \n",
    "                    int(float(x)) for\n",
    "                        x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "                \n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "                \n",
    "        # the first column is where the image files are\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path) # we are using PIL to open\n",
    "        # the images are converted to a tensor for the sake of doing\n",
    "        #  transformations with Pytorch if needed.\n",
    "        boxes = torch.tensor(boxes)\n",
    "        \n",
    "        # transform is used if we are doing data augmentation\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "        # the annotated images will only have ONE bounding box per cell, so\n",
    "        # only C + 5 [0:C-1 (class probabilities), objectness, x, y, w, h].\n",
    "        # The *self.B is only there to match the size of the tensor in other\n",
    "        # parts of the code, they don't contain any data for now.\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "\n",
    "        # we need to convert everything to fit this label matrix. The\n",
    "        # bounding boxes in the read images are relative to the entire\n",
    "        # image, but YOLO needs the boxes relative to cells, so a\n",
    "        # a conversion is necessary.\n",
    "        for box in boxes:\n",
    "            # after the transformation, the boxes are converted\n",
    "            # to list again.\n",
    "            class_label, x, y, width, height = box.tolist()       \n",
    "            # this is to make absolute sure the class_label IS an int\n",
    "            class_label = int(class_label)\n",
    "            # we need to see which cell row and cell column each box belongs to.\n",
    "            # to convert this, we multiply the values by the number of cells S to\n",
    "            # escale it appropriately, then by taking the intenger of that the \n",
    "            # number is rounded down to the row i for y and column j for x that\n",
    "            # the box belongs to.\n",
    "            i = int(self.S * y) # row it belongs to\n",
    "            j = int(self.S * x) # column it belongs to            \n",
    "            # the x and y relative to the cell are obtained by subtracting the\n",
    "            # column for x and row for y from the x and y values that got escaled \n",
    "            # by S. In doing so, we only get the decimal part between 0 and 1 \n",
    "            # that indicates where in the cell the centerpoints are.\n",
    "            x_cell = self.S * x - j\n",
    "            y_cell = self.S * y - i\n",
    "            # the original width and height values were relative to the entire\n",
    "            # image, so to get them relative to the cell we just multiply them\n",
    "            # by the number of cells S we split it by.\n",
    "            width_cell  = width*self.S\n",
    "            height_cell = height*self.S\n",
    "            # the objectness is used to determine if there is an object\n",
    "            # in [i, j]. If there is currently no object there (0), we\n",
    "            # are going to use it.\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "                # the boox coordinates are now converted to tensor\n",
    "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
    "                # then added to their position (C+1:C+5) in the label_matrix, this is after\n",
    "                # the class probabibilites and the objectness.\n",
    "                label_matrix[i, j, (self.C+1):(self.C+5)] = box_coordinates\n",
    "                # this sets the index of the class label to 1, to signify\n",
    "                # what class the box represents.\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "                \n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1198d71-2438-4050-9963-6ed4a3995e52",
   "metadata": {},
   "source": [
    " <center><img src='fig/nonmax-1.png' width=\"600\"><center>\n",
    "\n",
    "Every bounding box has a probability score between 0 and 1 for each class independently that corresponds to how likely it is that there is an object of that class in that bounding box. Non-max suppression means that we choose the box with the highest probability score.\n",
    "\n",
    "The intersection of union is used between this box that has the highest probability score and other overlapping boxes. If the IOU returns a value that is higher than a certain threshold, those other overlapping boxes are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3778440a-d796-47aa-ac13-44fade9b69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "non max supression\n",
    "'''\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes, cleaning up overlapping bounding boxes.\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # bboxes = [[class_label, class_probability, x, y, w, h]]\n",
    "    # the bboxes parameter has to be a list for this to work.\n",
    "    assert type(bboxes) == list\n",
    "    \n",
    "    # this only keeps the boxes that have probability above a given threshold.\n",
    "    # a low probability of containing that class' object is immediately pruned.\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    # this sorts the bounding boxes with the highest probability at the beginning.\n",
    "    # we want to choose the box with the highest probability first.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    # empty list that will store the bounding boxes after the nms.\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        # this pops the box with the highest score\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        \n",
    "        # this updates the list of the bounding boxes\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            # if the class of the box is not the same as the class of the\n",
    "            # chosen box, we don't want to compare them. So just for that\n",
    "            # condition, we want to keep that bounding box to be tested\n",
    "            # with its respective class later.\n",
    "            if box[0] != chosen_box[0]\n",
    "            # if they are the same class, we want to compare them.\n",
    "            or intersection_over_union(\n",
    "                # 2: because we are only passing the [x, y, w, h] params\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "                ) < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd4dcb-59f9-4baf-bc8d-7a51200f1bd1",
   "metadata": {},
   "source": [
    "<center> <img src='fig/mAP.png' width=\"600\"> <center>\n",
    "    \n",
    "The mean average precision is the most common metric used in deep learning to evaluate object detection models.\n",
    "    \n",
    "<center> <img src='fig/pre_rec.png' width=\"600\"> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd43388-6741-4ad8-8ee3-2509067f02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mean average precision\n",
    "'''\n",
    "\n",
    "def mean_average_precision(pred_boxes, true_boxes, num_classes, iou_threshold=0.5, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c6acc24-745a-423c-b245-e3a8ce1c6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "box utils. Those functions are responsible for converting the bounding boxes.\n",
    "'''\n",
    "\n",
    "def cellboxes_to_boxes(out, S, B, C):\n",
    "    '''\n",
    "    converts the bounding boxes relative to the cell to be relative to the entire image.\n",
    "    Parameters:\n",
    "        out (list): list with the boxes relative to their cell.\n",
    "        S (int): the dimension of the grid used for the cells.\n",
    "    Returns: \n",
    "        list: list with the x, y, w and h relative to the entire image.\n",
    "    '''\n",
    "    converted_pred = convert_cellboxes(out, S=S, B=B, C=C).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def convert_cellboxes(predictions, S, B, C):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + B * 5)\n",
    "    bboxes1 = predictions[..., (C+1):(C+5)]\n",
    "    bboxes2 = predictions[..., (C+6):(C+10)]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., (C+5)].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., (C+5)]).unsqueeze(-1)\n",
    "                                \n",
    "    converted_preds = torch.cat(\n",
    "                                (predicted_class,\n",
    "                                 best_confidence,\n",
    "                                 converted_bboxes\n",
    "                                ),\n",
    "                                dim=-1\n",
    "                               )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def get_bboxes(loader,\n",
    "               model,\n",
    "               iou_threshold,\n",
    "               threshold,\n",
    "               split_size,\n",
    "               num_boxes,\n",
    "               num_classes,\n",
    "               pred_format=\"cells\",\n",
    "               box_format=\"midpoint\",\n",
    "               device=\"cuda\",\n",
    "              ):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(out = labels, S = split_size, B = num_boxes, C = num_classes)\n",
    "        bboxes = cellboxes_to_boxes(out = predictions, S = split_size, B = num_boxes, C = num_classes)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes = bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "\n",
    "            #if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "class early_stopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    a certain amount of epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience, min_delta, counter = 0, best_loss = None, early_stop = False):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = counter\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "    def params_getter(self):\n",
    "        return self.patience, self.min_delta, self.counter, self.best_loss, self.early_stop\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecbe6e6-b3ce-4ec2-9139-2a617fddae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot functions\n",
    "'''\n",
    "# this is the function that I need to change to be able to plot\n",
    "# the class names\n",
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle potch\n",
    "    for box in boxes:\n",
    "        # the class name is obtained using the class index\n",
    "        class_name = CLASSES[int(box[0])]\n",
    "        class_prob = \"{:.3f}\".format(box[1])\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(upper_left_x*width, upper_left_y*height, class_name, color=\"r\")\n",
    "        ax.text(upper_left_x*width, upper_left_y*height - 20, class_prob, color=\"r\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def loss_graph(show = False):\n",
    "    fig, ax = plt.subplots(facecolor='white')\n",
    "    epochs = [i for i in range(0, len(val_loss))]\n",
    "    plt.plot(epochs, val_loss, 'b--', label = 'Validation loss')\n",
    "    plt.plot(epochs, train_loss, 'r--', label = 'Training loss')\n",
    "    plt.axvline(x=loss_checkpoint, color='black', linestyle='--', label = f'Val loss checkpoint (patience = {VAL_LOSS_PATIENCE})')\n",
    "    plt.axvline(x=mAP_checkpoint, color='gray', linestyle='--', label = 'Train mAP checkpoint (mAP >= 0.9)')\n",
    "    ax.set(xlabel='Epochs', ylabel='Loss', title=\"Loss x Epochs\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    fig.savefig(\"Loss x Epochs - Training vs Validation\" + GRAPH_NAME + \".png\")\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def mAP_graph(show = False):\n",
    "    fig, ax = plt.subplots(facecolor='white')\n",
    "    epochs = [i for i in range(0, len(val_mAP))]\n",
    "    plt.plot(epochs, val_mAP, 'b--', label = 'Validation mAP')\n",
    "    plt.plot(epochs, train_mAP, 'r--', label = 'Training mAP')\n",
    "    plt.axvline(x=loss_checkpoint, color='black', linestyle='--', label = f'Val mAP checkpoint (patience = {VAL_LOSS_PATIENCE})')\n",
    "    plt.axvline(x=mAP_checkpoint, color='gray', linestyle='--', label = 'Train mAP checkpoint (mAP >= 0.9)')\n",
    "    ax.set(xlabel='Epochs', ylabel='mAP', title=\"mAP x Epochs\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    fig.savefig(\"mAP x Epochs - Training vs Validation\" + GRAPH_NAME + \".png\")\n",
    "    if show == True:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4296a10-2967-41a8-a4be-4175fafac650",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "checkpoint functions\n",
    "'''\n",
    "def save_checkpoint(state, filename=\"final.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a667dbc-f6f4-4ec0-9aaa-2235263f5585",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train\n",
    "'''\n",
    "# to always load the same dataset\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "#LEARNING_RATE = 2e-5\n",
    "#LEARNING_RATE = 0.0013\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "# dropout and weight decay are both 0 here for now, but\n",
    "# we need to change them to something else if we intend \n",
    "# to actually train the model, use data augmentation and\n",
    "# and whatnot.\n",
    "# We put weight_decay to 0 to OVERFIT the image on purpose,\n",
    "# just to check if the model IS working as intended.\n",
    "#WEIGHT_DECAY = 0\n",
    "WEIGHT_DECAY = 0.0005\n",
    "# WIDTH and HEIGHT must be multiples of 7?\n",
    "WIDTH = 448 # 640\n",
    "HEIGHT = 448 # 480 \n",
    "EPOCHS = 500\n",
    "NUM_WORKERS = 0 # I'd use 2 if the computer allowed.\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"aug_inplace_save.pth.tar\"\n",
    "SAVE_MODEL_FILE = \"aug_inplace\"\n",
    "#LOAD_MODEL_FILE = \"640x480_save.pth.tar\"\n",
    "#SAVE_MODEL_FILE = \"640x480\"\n",
    "GRAPH_NAME = \" for in-place aug input\"\n",
    "IMG_DIR = \"./Data/aug_images/\"\n",
    "LABEL_DIR = \"./Data/aug_labels/\"\n",
    "NUM_BOXES = 2\n",
    "SPLIT_SIZE = 7\n",
    "NUM_CLASSES = 4\n",
    "VAL_LOSS_PATIENCE = 50 # 30 was too low, maybe 50 next time?\n",
    "VAL_LOSS_DELTA = 20 # 2 was too low\n",
    "# reading CSV file for class names\n",
    "CLASSES = pd.read_csv(\"names.csv\", names = [\"class\"])\n",
    "CLASSES = CLASSES[\"class\"].tolist()\n",
    "AUG_INPLACE = True\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        # the transform only operates on the images, we don't\n",
    "        # want to transform the bounding boxes. They are already\n",
    "        # relative to the image in ratios, regardless of its size.\n",
    "        # IF the transforms used were also for data augmentation,\n",
    "        # then we should do t(img, bboxes). This will be implemented\n",
    "        # in the future perhaps.\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "# the input images will be resized to 448x448\n",
    "if AUG_INPLACE == \"True\":\n",
    "    scale = 1.1\n",
    "    # p is the probability of applying a given transform\n",
    "    # longestMaxSize keeps the aspect ratio of the image\n",
    "    # PadIfNeeded will be used pad the empty space\n",
    "    # 1.1 scale is used to minimize the padded area\n",
    "    # RandomCrop is applied on top of that to make the resulting image be\n",
    "    # within the input size of the network, but each epoch will have variations.\n",
    "    train_transforms = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=int(max(WIDTH, HEIGHT) * scale)), \n",
    "            A.PadIfNeeded(\n",
    "                min_height=int(HEIGHT * scale),\n",
    "                min_width=int(WIDTH * scale),\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "            ),\n",
    "            A.RandomCrop(width=WIDTH, height=HEIGHT),\n",
    "            A.Multiply((0.7, 1.3), p=0.5),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.ShiftScaleRotate(\n",
    "                        rotate_limit=10, p=0.5, border_mode=cv2.BORDER_REFLECT_101\n",
    "                    ),\n",
    "                    A.Perspective (scale=(0, 0.1), p=0.5),\n",
    "                ],\n",
    "                p=1.0,\n",
    "            ),\n",
    "            A.Cutout(num_holes=4, max_h_size=8, max_w_size=8, fill_value=0, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.8, label_fields=[],),\n",
    "    )\n",
    "    test_transforms = A.Compose(\n",
    "            [\n",
    "                A.LongestMaxSize(max_size= max(IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "                A.PadIfNeeded(\n",
    "                    min_height=IMAGE_HEIGHT, min_width=IMAGE_WIDTH, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),)\n",
    "else:\n",
    "    train_transforms = Compose([transforms.Resize((WIDTH, HEIGHT)), transforms.ToTensor(),])\n",
    "    test_transforms  = Compose([transforms.Resize((WIDTH, HEIGHT)), transforms.ToTensor(),])\n",
    "    \n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    # leave true is for the progress bar\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    mean_loss = sum(mean_loss)/len(mean_loss)\n",
    "    print(f\"Training mean loss was {mean_loss}\")\n",
    "    return mean_loss\n",
    "\n",
    "def validation_fn(valid_loader, model, loss_fn):\n",
    "    # Settings\n",
    "    model.eval()\n",
    "    loop = tqdm(valid_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    # Test validation data\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            mean_loss.append(loss.item())\n",
    "            \n",
    "            # update the progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    mean_loss = sum(mean_loss)/len(mean_loss)\n",
    "    print(f\"Validation mean loss was {mean_loss}\")\n",
    "    model.train()\n",
    "    return mean_loss\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_mAP = []\n",
    "val_mAP = []\n",
    "loss_checkpoint = 0\n",
    "mAP_checkpoint = 0\n",
    "early_stopping_params = [[VAL_LOSS_PATIENCE, VAL_LOSS_DELTA, 0, 0, False]]\n",
    "early_stopping_params = pd.DataFrame(early_stopping_params, columns=[\"patience\", \"min_delta\", \"counter\", \"best_loss\", \"early_stop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ce5c76-e266-45be-b3d9-ec990bff2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Flags to control saving the model for early stopping\n",
    "    loss_stopped = False\n",
    "    train_precision_stopped = False\n",
    "    global loss_checkpoint\n",
    "    global mAP_checkpoint\n",
    "    global early_stopping_params\n",
    "   \n",
    "    model = Yolov1(split_size=SPLIT_SIZE, num_boxes=NUM_BOXES, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    #optimizer = optim.SGD(\n",
    "    #    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    #)\n",
    "    loss_fn = YoloLoss(split_size = SPLIT_SIZE, num_boxes = NUM_BOXES, num_classes = NUM_CLASSES)\n",
    "    \n",
    "    loaded_epoch = 0\n",
    "    if LOAD_MODEL:\n",
    "        checkpoint = torch.load(LOAD_MODEL_FILE)\n",
    "        load_checkpoint(checkpoint, model, optimizer)\n",
    "        loaded_epoch = checkpoint['epoch']\n",
    "        loss_fn = checkpoint['loss']\n",
    "        early_stopping_params = pd.read_csv('early_stopping_params_' + SAVE_MODEL_FILE + '.csv')\n",
    "        loss_stopping = early_stopping(patience   = early_stopping_params[\"patience\"], \n",
    "                                       min_delta  = early_stopping_params[\"min_delta\"], \n",
    "                                       counter    = early_stopping_params[\"counter\"],\n",
    "                                       best_loss  = early_stopping_params[\"best_loss\"],\n",
    "                                       early_stop = early_stopping_params[\"early_stop\"])\n",
    "        train_loss = pd.read_csv('train_loss_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        val_loss = pd.read_csv('val_loss_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        train_mAP = pd.read_csv('train_mAP_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        val_mAP = pd.read_csv('val_mAP_' + SAVE_MODEL_FILE + '.csv', squeeze = True).values.tolist()\n",
    "        \n",
    "    else:\n",
    "        loss_stopping = early_stopping(patience = VAL_LOSS_PATIENCE, min_delta = VAL_LOSS_DELTA)\n",
    "\n",
    "\n",
    "    train_dataset = VOCDataset(\n",
    "        \"./Data/train_data.csv\",\n",
    "        transform=train_transforms,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "        split_size = SPLIT_SIZE, \n",
    "        num_boxes = NUM_BOXES, \n",
    "        num_classes = NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    validation_dataset = VOCDataset(\n",
    "        \"./Data/aug_validation.csv\", \n",
    "        transform=test_transforms, \n",
    "        img_dir=IMG_DIR, \n",
    "        label_dir=LABEL_DIR, \n",
    "        split_size = SPLIT_SIZE,\n",
    "        num_boxes = NUM_BOXES,\n",
    "        num_classes = NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        # if there is a batch and there are only two examples,\n",
    "        # we don't want to do an update step on that, so\n",
    "        # drop_last SHOULD be True. Check why I can't make it so.\n",
    "        drop_last=True, # if True gets a division by 0\n",
    "    )\n",
    "\n",
    "    validation_loader = DataLoader(\n",
    "        dataset=validation_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    #summary(model, (3, WIDTH, HEIGHT))\n",
    "    #return\n",
    "    for epoch in range(loaded_epoch, EPOCHS):\n",
    "        '''\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(DEVICE)            \n",
    "            for idx in range(8):\n",
    "              bboxes = cellboxes_to_boxes(out = model(x), S = SPLIT_SIZE, B = NUM_BOXES, C = NUM_CLASSES)\n",
    "              bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "              plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "            #import sys\n",
    "            #sys.exit()\n",
    "        '''\n",
    "        print(\"_\"*50)\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        \n",
    "        print(\"\\nTrain set: \")\n",
    "        pred_boxes, target_boxes = get_bboxes(loader        = train_loader, \n",
    "                                              model         = model, \n",
    "                                              iou_threshold = 0.5, \n",
    "                                              threshold     = 0.4, \n",
    "                                              split_size    = SPLIT_SIZE, \n",
    "                                              num_boxes     = NUM_BOXES, \n",
    "                                              num_classes   = NUM_CLASSES,\n",
    "                                              box_format    = \"midpoint\",\n",
    "                                              device        = DEVICE\n",
    "                                             )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes    = pred_boxes,\n",
    "                                               true_boxes    = target_boxes,\n",
    "                                               iou_threshold = 0.5,\n",
    "                                               box_format    = \"midpoint\",\n",
    "                                               num_classes   = NUM_CLASSES\n",
    "                                              )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "        train_mAP.append(mean_avg_prec)\n",
    "        if(epoch%10 == 0):\n",
    "            checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \".pth.tar\")\n",
    "            \n",
    "        if ((mean_avg_prec >= 0.90) and (train_precision_stopped == False)):\n",
    "            mAP_checkpoint = epoch\n",
    "            checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            }\n",
    "            print(f\"Saving checkpoint due to mAP reaching threshold 0.90 at epoch {mAP_checkpoint}.\")\n",
    "            print(epoch)\n",
    "            save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \"train_mAP_stopped.pth.tar\")\n",
    "            train_precision_stopped = True\n",
    "            \n",
    "            \n",
    "        train_loss.append(train_fn(train_loader, model, optimizer, loss_fn))\n",
    "        \n",
    "        print(\"\\nValidation set: \")\n",
    "        pred_boxes, target_boxes = get_bboxes(loader        = validation_loader, \n",
    "                                              model         = model, \n",
    "                                              iou_threshold = 0.5, \n",
    "                                              threshold     = 0.4, \n",
    "                                              split_size    = SPLIT_SIZE, \n",
    "                                              num_boxes     = NUM_BOXES, \n",
    "                                              num_classes   = NUM_CLASSES,\n",
    "                                              box_format    = \"midpoint\",\n",
    "                                              device        = DEVICE\n",
    "                                             )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes    = pred_boxes,\n",
    "                                               true_boxes    = target_boxes,\n",
    "                                               iou_threshold = 0.5,\n",
    "                                               box_format    = \"midpoint\",\n",
    "                                               num_classes   = NUM_CLASSES\n",
    "                                              )\n",
    "        \n",
    "        print(f\"Validation mAP: {mean_avg_prec}\")\n",
    "        val_mAP.append(mean_avg_prec)\n",
    "        val_loss.append(validation_fn(validation_loader, model, loss_fn))\n",
    "        \n",
    "        if loss_stopped == False:\n",
    "            loss_stopping(val_loss[-1])\n",
    "            (early_stopping_params[\"patience\"], \n",
    "            early_stopping_params[\"min_delta\"], \n",
    "            early_stopping_params[\"counter\"],\n",
    "            early_stopping_params[\"best_loss\"],\n",
    "            early_stopping_params[\"early_stop\"]) = loss_stopping.params_getter()\n",
    "            early_stopping_params.to_csv('early_stopping_params_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "            if (loss_stopping.early_stop == True):\n",
    "                loss_checkpoint = epoch\n",
    "                checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                'loss': loss_fn,\n",
    "                }\n",
    "                print(f\"Saving checkpoint due to validation loss at epoch {loss_checkpoint}.\")\n",
    "                print(epoch)\n",
    "                save_checkpoint(checkpoint, filename=SAVE_MODEL_FILE + \"loss_stopped.pth.tar\")\n",
    "                loss_stopped = True\n",
    "            \n",
    "            #break\n",
    "        dataframe = pd.DataFrame(train_loss)\n",
    "        dataframe.to_csv('train_loss_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        dataframe = pd.DataFrame(val_loss) \n",
    "        dataframe.to_csv('val_loss_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        dataframe = pd.DataFrame(train_mAP)\n",
    "        dataframe.to_csv('train_mAP_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        dataframe = pd.DataFrame(val_mAP) \n",
    "        dataframe.to_csv('val_mAP_' + SAVE_MODEL_FILE + '.csv', index=False)\n",
    "        loss_graph()\n",
    "        mAP_graph()\n",
    "    loss_graph(show = True)\n",
    "    mAP_graph(show = True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692a0e55-e798-426e-b571-2ec823f5136f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Epoch: 0\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.0\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562f3e59237c40a39e7507dc325599df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 1138.8827178955078\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd5f7148c1c4fa3b370c7a9154a3961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 1140.5450032552083\n",
      "__________________________________________________\n",
      "Epoch: 1\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fbe86b89054721bcfdea1667b4a3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 619.4419982910156\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9945cb19de46b187045ea2092b3424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 901.0576477050781\n",
      "__________________________________________________\n",
      "Epoch: 2\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b94bd63074ed88c91a51d3dc999f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 465.03074951171874\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 8.258913112513255e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6b841c66e0497db5085dd7175c5e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 834.6329650878906\n",
      "__________________________________________________\n",
      "Epoch: 3\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.00046205773833207786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e1991dcd794a7599c95370aefa25e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 370.30051574707034\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 5.1829621952492744e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76d168eba324b62985de9f637ae99f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 842.4395345052084\n",
      "INFO: Early stopping counter 1 of 50\n",
      "__________________________________________________\n",
      "Epoch: 4\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.09962793439626694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b871901a1b4aabb405c44923121a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 297.3417739868164\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 1.776710087142419e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43eb2a2de03d4b24840e108d7efc5cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 848.5513000488281\n",
      "INFO: Early stopping counter 2 of 50\n",
      "__________________________________________________\n",
      "Epoch: 5\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.24283462762832642\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380983ee0ff041e68215fde79cf81ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 250.26298980712892\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 2.8601232770597562e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74338b0f7f4b409eaadc32c8dfbfefc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 833.7277119954427\n",
      "INFO: Early stopping counter 3 of 50\n",
      "__________________________________________________\n",
      "Epoch: 6\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.33238255977630615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af00079f5d0843c6abb7f97717fe22cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 205.23883285522462\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 3.477002974250354e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f69feeb19c42ae96e159282de637da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 833.765879313151\n",
      "INFO: Early stopping counter 4 of 50\n",
      "__________________________________________________\n",
      "Epoch: 7\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.36712542176246643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5296486645634409911fc63a1e01617d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 196.8385238647461\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 4.579641972668469e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da21714298d4c3597f7b98f29730fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 835.9829610188802\n",
      "INFO: Early stopping counter 5 of 50\n",
      "__________________________________________________\n",
      "Epoch: 8\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.390237033367157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c1f2d4714c44319d8cbbb7cdef0ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 188.20002822875978\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 0.00010180051322095096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab304872de04b0bb6d99a3ed89f16f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 843.3299560546875\n",
      "INFO: Early stopping counter 6 of 50\n",
      "__________________________________________________\n",
      "Epoch: 9\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.4020307660102844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a409ebae2244a6852ff5b67caf16fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 194.12249450683595\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 4.306354821892455e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d303f198ee44b2d904e1c358755f067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 853.198008219401\n",
      "INFO: Early stopping counter 7 of 50\n",
      "__________________________________________________\n",
      "Epoch: 10\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.43650439381599426\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524081dee61747dab898428ae9fda7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 170.55864868164062\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 3.813937655650079e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bed6e3f1c594359b8104f82f26f3a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 869.8562723795573\n",
      "INFO: Early stopping counter 8 of 50\n",
      "__________________________________________________\n",
      "Epoch: 11\n",
      "\n",
      "Train set: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-adcc9fc24977>:44: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(facecolor='white')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.4152370095252991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1318e20b5a964cf882543b978188d025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 169.9323860168457\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 3.585771764846868e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ba3b5378f4c9e9e0509f70cf5ac57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 858.1216430664062\n",
      "INFO: Early stopping counter 9 of 50\n",
      "__________________________________________________\n",
      "Epoch: 12\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.45140624046325684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c0ab7e48cc4e3fac87f1ba51813658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 157.65988464355468\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 0.0001007549071800895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4a890a29094cbeb1a80782032d58f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 836.8539530436198\n",
      "INFO: Early stopping counter 10 of 50\n",
      "__________________________________________________\n",
      "Epoch: 13\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.49107396602630615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc869d9d54a4851bf872e0ccb20399d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 164.79972763061522\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 1.951490958163049e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24546ecebadd45e58debb0b6ce973b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 840.6087341308594\n",
      "INFO: Early stopping counter 11 of 50\n",
      "__________________________________________________\n",
      "Epoch: 14\n",
      "\n",
      "Train set: \n",
      "Train mAP: 0.46049565076828003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dce79dd3cce484ea39d023cc47a0082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss was 158.17764587402343\n",
      "\n",
      "Validation set: \n",
      "Validation mAP: 1.397442065353971e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2dcc22ad39e40769b4c0335cc16d5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss was 816.4307759602865\n",
      "INFO: Early stopping counter 12 of 50\n",
      "__________________________________________________\n",
      "Epoch: 15\n",
      "\n",
      "Train set: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-73aacb47edda>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTrain set: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         pred_boxes, target_boxes = get_bboxes(loader        = train_loader, \n\u001b[0m\u001b[1;32m     94\u001b[0m                                               \u001b[0mmodel\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                                               \u001b[0miou_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-91180d0d040d>\u001b[0m in \u001b[0;36mget_bboxes\u001b[0;34m(loader, model, iou_threshold, threshold, split_size, num_boxes, num_classes, pred_format, box_format, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtrain_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-52cf13cd1bcf>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# transform is used if we are doing data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# the annotated images will only have ONE bounding box per cell, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# only C + 5 [0:C-1 (class probabilities), objectness, x, y, w, h].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-31b4173d26e7>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img, bboxes)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# in the future perhaps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \"\"\"\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7f6b8b40cca0> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                        else suppress())\n\u001b[1;32m   2229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2780\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   2781\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2919\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2921\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2923\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks_to_draw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;31m# scale up the axis label box to also find the neighbors, not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    300\u001b[0m         for artist in [self.gridline, self.tick1line, self.tick2line,\n\u001b[1;32m    301\u001b[0m                        self.label1, self.label2]:\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    833\u001b[0m                                      .get_transformed_points_and_affine())\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 tpath, affine = (self._get_transformed_path()\n\u001b[0m\u001b[1;32m    836\u001b[0m                                  .get_transformed_points_and_affine())\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mget_transformed_points_and_affine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0mperformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \"\"\"\n\u001b[0;32m-> 2737\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_revalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformed_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_revalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2720\u001b[0m                 or self._transformed_path is None):\n\u001b[1;32m   2721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformed_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_path_non_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformed_points\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m                 Path._fast_from_codes_and_verts(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mtransform_path_non_affine\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \"\"\"\n\u001b[1;32m   1595\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_non_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_from_codes_and_verts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_angles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradians\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpushoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/path.py\u001b[0m in \u001b[0;36m_fast_from_codes_and_verts\u001b[0;34m(cls, verts, codes, internals_from)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[1;32m    176\u001b[0m         \u001b[0mpth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mpth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mpth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mpth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce0ee4-7411-4a03-b2cf-c2c31416f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph(show = True)\n",
    "mAP_graph(show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a14c05-36d1-4e42-9c6d-b87f9103c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "18:48\n",
    "19:53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93b1df-12b3-49f8-a6a2-3d1568109eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
